{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn-7qo07aFoy"
      },
      "outputs": [],
      "source": [
        "# importing functions we'll need for aggregations and date handling\n",
        "from pyspark.sql.functions import (\n",
        "    col, sum as _sum, avg, to_date, date_trunc, max as _max\n",
        ")\n",
        "\n",
        "# file path from ADLS Gen2 storage\n",
        "file_path = \"abfss://nyctaxi@saptadipnyctaxi.dfs.core.windows.net/raw/yellow_tripdata_2020-01.csv\"\n",
        "\n",
        "# reading the CSV file with comma as delimiter\n",
        "df = spark.read.option(\"delimiter\", \",\") \\\n",
        "    .option(\"header\", True) \\\n",
        "    .option(\"inferSchema\", True) \\\n",
        "    .csv(file_path)\n",
        "\n",
        "# checking schema and a few rows just to confirm it loaded fine\n",
        "df.printSchema()\n",
        "df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll now calculate revenue by summing up all the fare-related columns\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_with_revenue = df.withColumn(\n",
        "    \"Revenue\",\n",
        "    col(\"fare_amount\") +\n",
        "    col(\"extra\") +\n",
        "    col(\"mta_tax\") +\n",
        "    col(\"tip_amount\") +\n",
        "    col(\"tolls_amount\") +\n",
        "    col(\"improvement_surcharge\") +\n",
        "    col(\"congestion_surcharge\")\n",
        ")\n",
        "\n",
        "# quick peek to see if the revenue column looks okay\n",
        "df_with_revenue.select(\"VendorID\", \"fare_amount\", \"tip_amount\", \"congestion_surcharge\", \"Revenue\").show(10)\n"
      ],
      "metadata": {
        "id": "spPMkCSiaIk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking total number of passengers per pickup location\n",
        "from pyspark.sql.functions import sum as _sum\n",
        "\n",
        "pickup_passenger_df = df_with_revenue.groupBy(\"PULocationID\") \\\n",
        "    .agg(_sum(\"passenger_count\").alias(\"total_passengers\")) \\\n",
        "    .orderBy(\"total_passengers\", ascending=False)\n",
        "\n",
        "pickup_passenger_df.show(10)\n",
        "\n",
        "# now same thing, but for dropoff locations\n",
        "dropoff_passenger_df = df_with_revenue.groupBy(\"DOLocationID\") \\\n",
        "    .agg(_sum(\"passenger_count\").alias(\"total_passengers\")) \\\n",
        "    .orderBy(\"total_passengers\", ascending=False)\n",
        "\n",
        "dropoff_passenger_df.show(10)\n"
      ],
      "metadata": {
        "id": "_za_Mj7yaLMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating average fare, total amount, and revenue for each vendor\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "vendor_stats_df = df_with_revenue.groupBy(\"VendorID\").agg(\n",
        "    avg(\"fare_amount\").alias(\"avg_fare\"),\n",
        "    avg(\"total_amount\").alias(\"avg_total_amount\"),\n",
        "    avg(\"Revenue\").alias(\"avg_revenue\")\n",
        ")\n",
        "\n",
        "vendor_stats_df.show()\n"
      ],
      "metadata": {
        "id": "ps_iFUjDaNvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we want to see how payment types are used over time, so we bucket time to the minute\n",
        "from pyspark.sql.functions import date_trunc, count\n",
        "\n",
        "payment_over_time = df_with_revenue.withColumn(\n",
        "    \"pickup_minute\", date_trunc(\"minute\", col(\"tpep_pickup_datetime\"))\n",
        ")\n",
        "\n",
        "# counting number of payments per minute for each payment type\n",
        "moving_payment_count = payment_over_time.groupBy(\"pickup_minute\", \"payment_type\") \\\n",
        "    .agg(count(\"*\").alias(\"payment_count\")) \\\n",
        "    .orderBy(\"pickup_minute\", ascending=True)\n",
        "\n",
        "moving_payment_count.show(10)\n"
      ],
      "metadata": {
        "id": "4HW1RyIoaQWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering for one specific day (15th Jan 2020) to check vendor performance\n",
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "filtered_date = df_with_revenue.withColumn(\"trip_date\", to_date(\"tpep_pickup_datetime\")) \\\n",
        "    .filter(col(\"trip_date\") == \"2020-01-15\")\n",
        "\n",
        "# getting total revenue, passengers, and distance per vendor\n",
        "top_vendors = filtered_date.groupBy(\"VendorID\").agg(\n",
        "    _sum(\"Revenue\").alias(\"total_revenue\"),\n",
        "    _sum(\"passenger_count\").alias(\"total_passengers\"),\n",
        "    _sum(\"trip_distance\").alias(\"total_distance\")\n",
        ").orderBy(\"total_revenue\", ascending=False)\n",
        "\n",
        "# showing top 2 vendors on that day\n",
        "top_vendors.show(2)\n"
      ],
      "metadata": {
        "id": "R68RLRTqaTWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we'll find the most frequent pickup-dropoff route based on passenger count\n",
        "popular_routes_df = df_with_revenue.groupBy(\"PULocationID\", \"DOLocationID\").agg(\n",
        "    _sum(\"passenger_count\").alias(\"total_passengers\")\n",
        ").orderBy(\"total_passengers\", ascending=False)\n",
        "\n",
        "popular_routes_df.show(10)\n"
      ],
      "metadata": {
        "id": "SP8DbcwbaVFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# figuring out where people were picked up most in the last 10 seconds of data\n",
        "from pyspark.sql.functions import max as _max\n",
        "\n",
        "# first get the latest timestamp from the dataset\n",
        "latest_ts = df_with_revenue.agg(_max(\"tpep_pickup_datetime\")).first()[0]\n",
        "print(\"Latest pickup timestamp in data:\", latest_ts)\n",
        "\n",
        "# subtract 10 seconds from the latest timestamp\n",
        "from pyspark.sql.functions import col\n",
        "window_start = latest_ts.replace(second=latest_ts.second - 10)\n",
        "\n",
        "# filter trips in the last 10 second window\n",
        "recent_pickups = df_with_revenue.filter(\n",
        "    (col(\"tpep_pickup_datetime\") > window_start) &\n",
        "    (col(\"tpep_pickup_datetime\") <= latest_ts)\n",
        ")\n",
        "\n",
        "# count passengers picked up at each location in that tiny window\n",
        "hotspots = recent_pickups.groupBy(\"PULocationID\").agg(\n",
        "    _sum(\"passenger_count\").alias(\"passenger_count\")\n",
        ").orderBy(\"passenger_count\", ascending=False)\n",
        "\n",
        "hotspots.show()\n"
      ],
      "metadata": {
        "id": "57cTE19EaXNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving each of the query results to the 'processed' folder in ADLS\n",
        "\n",
        "# Revenue Data\n",
        "df_with_revenue.write.mode(\"overwrite\").parquet(\"abfss://nyctaxi@saptadipnyctaxi.dfs.core.windows.net/processed/revenue/\")\n",
        "\n",
        "# Passenger by Pickup Area\n",
        "pickup_passenger_df.write.mode(\"overwrite\").parquet(\"abfss://nyctaxi@saptadipnyctaxi.dfs.core.windows.net/processed/passenger_by_pickup_area/\")\n",
        "\n",
        "# Vendor-wise Averages\n",
        "vendor_stats_df.write.mode(\"overwrite\").parquet(\"abfss://nyctaxi@saptadipnyctaxi.dfs.core.windows.net/processed/vendor_averages/\")\n",
        "\n",
        "# Payment Type Over Time\n",
        "moving_payment_count.write.mode(\"overwrite\").parquet(\"abfss://nyctaxi@saptadipnyctaxi.dfs.core.windows.net/processed/payment_moving_count/\")\n",
        "\n",
        "# Top Vendors on Specific Date\n",
        "top_vendors.write.mode(\"overwrite\").parquet(\"abfss://nyctaxi@saptadipnyctaxi.dfs.core.windows.net/processed/top_vendors_by_date/\")\n",
        "\n",
        "# Most Popular Routes\n",
        "popular_routes_df.write.mode(\"overwrite\").parquet(\"abfss://nyctaxi@saptadipnyctaxi.dfs.core.windows.net/processed/popular_routes/\")\n",
        "\n",
        "# Recent Pickup Hotspots\n",
        "hotspots.write.mode(\"overwrite\").parquet(\"abfss://nyctaxi@saptadipnyctaxi.dfs.core.windows.net/processed/pickup_hotspots/\")\n"
      ],
      "metadata": {
        "id": "3eGm3QR1abZ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}